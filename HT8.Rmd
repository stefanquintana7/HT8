---
title: "Hoja de Trabajo 8"
author: "Pablo Quintana, Sofia Escobar, Wilfredo Gallegos"
date: "2023-04-28"
output: html_document
---

# Pregunta 1 y 2
```{r, echo=FALSE}
library(caret)
library(nnet)
library(dummy)
library(neuralnet)
library(Metrics)
library(gbm)

porcentaje<-0.7
datos <- read.csv("train.csv")

#Quitando na
datos[is.na(datos)] <- 0
datos$Id <- NULL


#Se agrega la variable clasificacion
datos$clasificacion <- ifelse(datos$SalePrice > 290000, "Caras", ifelse(datos$SalePrice>170000, "Intermedia", "Economica"))
datos$clasificacion <- as.factor(datos$clasificacion)

datos <- datos[,c(4,12,17,34,38,46,62,67,80,81)]

set.seed(123)
corte <- sample(nrow(datos),nrow(datos)*porcentaje)
train<-datos[corte,]
test<-datos[-corte,]
```


#Pregunta 3 y 4: Hacer dos redes neuronales diferentes y predicciones

```{r, echo=FALSE}
#Modelo 47
Rprof(memory.profiling = TRUE)
modelo.nn1 <- nnet(clasificacion~.,data = train, size=2, rang=0.1, decay=5e-4, maxit=200) 
Rprof(NULL)
l1<-summaryRprof(memory = "both")

prediccion1 <- as.data.frame(predict(modelo.nn1, newdata = test[,1:9]))
columnaMasAlta<-apply(prediccion1, 1, function(x) colnames(prediccion1)[which.max(x)])
test$prediccion1<-columnaMasAlta

cfm1 <-confusionMatrix(as.factor(test$prediccion1),as.factor(test$clasificacion))
test2 <-test


test$prediccion1<-NULL

#Modelo 2
Rprof(memory.profiling = TRUE)
modelo.nn2 <- nnet(clasificacion~.,data = train, size=20, rang=0.5, decay=2e-4, maxit=10) 
Rprof(NULL)
l2<-summaryRprof(memory = "both")

prediccion2 <- as.data.frame(predict(modelo.nn2, newdata = test[,1:9]))
columnaMasAlta<-apply(prediccion2, 1, function(x) colnames(prediccion2)[which.max(x)])
test$prediccion2<-columnaMasAlta

cfm2 <-confusionMatrix(as.factor(test$prediccion2),test$clasificacion)
test3 <-test
test$prediccion2<-NULL

```

##Pregunta 5: Matrices de confusión

## Mostrar matriz de confusion modelo 1
```{r, echo = FALSE}
cfm1
```

## Mostrar matriz de confusion modelo 2
```{r, echo = FALSE}
cfm2
```

##Pregunta 6: Efectividad y rendimiento de los modelos

## Mostrar rendimiento modelo 1
```{r, echo=FALSE}
print("Tiempo de entrenamiento modelo 1: ")
l1$sampling.time
```

## Mostrar rendimiento modelo 2
```{r, echo=FALSE}
print("Tiempo de entrenamiento modelo 2: ")
l2$sampling.time
```
Observamos que la efectividad de ambos modelos es bastante baja, incluso si la comparamos con modelos de otras hojas, hasta el momento es el modelo con menos tiempo de entrenamiento. 

## Pregunta 7: Sobreajuste en el modelo
### precision del modelo 1

```{r}
accuracy(test2$prediccion1, test$clasificacion)
```
Podemos observar que el modelo en general tiene una precision de 0.52, individualmente obtenemos que la presiciones son de 0.5, 0.504 y 0.49 para las clases caras, economica e intermedia respectivamente. Realmente podemos observar que existe bastante sobre ajuste, ya que el modelo propuesto no se ajusta suficientemente bien con los datos de entrenamineto y solo tiene un 50% de acertividad en la predicción.


```{r}
accuracy(test3$prediccion2, test$clasificacion)

```
De igual manera se obtienen exactamente los mismos resultados para el modelo 2 propuesto, lo cual nos indica que de todas maneras existe sobreajuste del modelo. Descartamos la posibilidad que esto sea debido a correlación entre variables ya que se ha analizado con anterioridad la correlacion entre todas las variables a utilizar. Esto puede suceder debido a que se esta tomando muy pocas variables en el modelo o estamos tomando algunas variables que no tienen representación y su significancia en el modelo altera la predicción.

##Pregunta 8: Tuneo de parametros


```{r}
train$Neighborhood <- as.factor(train$Neighborhood)
boosting_model <-gbm(clasificacion~.,
                     data=train,
                     n.trees = 500,
                     interaction.depth = 1,
                     shrinkage=0.1)
modelLookup("gbm")
set.seed(123)
auto_tune_boost <- train(clasificacion~.,
                     data=train,
                     method= "gbm",
                     metric = "Accuracy")
```


Evualuar nuestro modelo tuneado en el conjunto test

```{r}
auto_tune_predict <- predict(auto_tune_boost,
                             newdata = test)
auto_tune_predict
mean(auto_tune_predict==test$clasificacion)
```
El modelo tuneado tuvo una precision del 100% en el conjunto test.    


```{r}
library(tibble)
library(kableExtra)
tibble("Modelo" = c("Modelo 1 RNA parametros arbitrarios", "Modelo Tuneado con caret"),
       "Presicion"=c(0.52,1)) %>%
kable(caption = "Presiciones en el conjunto test",
      align = "lc")%>%
  kable_styling(bootstrap_options = "striped")%>%
  column_spec(1,bold = TRUE, color = "green")%>%
  column_spec(2,bold = TRUE, color = "royalblue")
```



## Pregunta 9 y 10: Modelos de regresión con la variable SalePrice
```{r, echo=FALSE}
#Modelo 1
Rprof(memory.profiling = TRUE)
modelo.nn3 <- nnet(SalePrice~.,data = train, size=2, rang=0.1, decay=5e-4, maxit=200) 
Rprof(NULL)
l3<-summaryRprof(memory = "both")

test$prediccion3<-predict(modelo.nn3, newdata = test)
prediccion3 <- as.data.frame(predict(modelo.nn3, newdata = test))
mean((test$prediccion3 - test$SalePrice))

plot(test$SalePrice, test$prediccion3,
     main="Neural network predictions vs actual",
     xlab="Actual")
test$prediccion3<-NULL

```

```{r, echo=FALSE}
#Modelo 2
Rprof(memory.profiling = TRUE)
modelo.nn4 <- nnet(SalePrice~.,data = train, size=20, rang=0.5, decay=2e-4, maxit=10) 
Rprof(NULL)
l4<-summaryRprof(memory = "both")

test$prediccion4<-predict(modelo.nn4, newdata = test)
prediccion4 <- as.data.frame(predict(modelo.nn4, newdata = test))
mean((test$prediccion4 - test$SalePrice))

plot(test$SalePrice, test$prediccion3,
     main="Neural network predictions vs actual",
     xlab="Actual")

test$prediccion4<-NULL

```

##Pregunta 11: Comparación de ambos modelos de regresión

Para las regresiones el segundo modelo presentó un R2 más cercano a 1, es decir, que estos datos se acercaban más a los resultados esperados. Por otro lado, analizando las gráficas de residuos se determinó que para ambos modelos tienen una distribución normal, esto quiere decir que los errores de regresión son aleatorios. Además, se observa que la media de residuos en ambos modelos se encuentra cercana a 0.

##Pregunta 12: Sobreajuste en los modelos de regresión
```{r}
#modelo 1
rmse(is.numeric(test$SalePrice), is.numeric(prediccion3))

#modelo 2
rmse(is.numeric(test$SalePrice), is.numeric(prediccion4))

```

##Pregunta 13: Tuneo de parametros en los modelos de regresión
```{r}

```

##Pregunta 14: Comparación de tiempo y predicción

```{r, echo=FALSE}
#modelo 1
print("Tiempo de entrenamiento modelo de regresión 1: ")
l3$sampling.time

#modelo 2
print("Tiempo de entrenamiento modelo de regresión 2: ")
l4$sampling.time
```
De acuerdo con las gráficas de residuos, que nos indican que ambos modelos tienen una distribución normal, la diferencia entre los R2 y el tiempo de entrenamiento no se logra determinar una diferencia significativa entre la calidad de ambos modelos. Por lo tanto, se recomienda explorar distintas topologías y otros solvers para determinar el modelo de regresión óptimo.

##Pregunta 15: Mejor modelo para clasificar (comparación con otras hojas)

Al observar los resultados se puede observar que otros modelos obtuvieron mejores resultados. Esto puede ser debido a que la cantidad de variables y neuronas que utilizamos para este modelo. En cuanto tiempo de ejecución se puede notar una gran diferencia entre los resultados y gran salto a comparación de de los otros algoritmos. Esto pudo haber sido por la cantidad de neuronas ya que cada una necesita obtener resultados de las capas.


##Pregunta 16: Mejor modelo para predecir (comparación con otras hojas)

A comparación de otros modelos este tiene un tiempo de predicción bastante bajo, ninguno sobrepasa los 0.05 lo cuál es positivo y da una ventaja frente a los otros algoritmos.

##Pregunta 17: Conclusión general 
